@article{Yang2014,
 author = {Yang, Yi and Zou, Hui},
 year = {2014},
 pages = {1-17},
 title = {Nonparametric multiple expectile regression via ER-Boost},
 volume = {85},
 journal = {Journal of Statistical Computation and Simulation},
 doi = {10.1080/00949655.2013.876024}
}

@article{Zeng2022,
 author = {Zeng, Huibin and Shao, Bilin and Bian, Genqing and Dai, Hongbin and Zhou, Fangyu},
 year = {2022},
 pages = {},
 title = {A hybrid deep learning approach by integrating extreme gradient boosting‐long short‐term memory with generalized autoregressive conditional heteroscedasticity family models for natural gas load volatility prediction},
 volume = {10},
 journal = {Energy Science \& Engineering},
 doi = {10.1002/ese3.1122}
}

@article{Thomas2018,
 author = {Thomas, Janek and Mayr, Andreas and Bischl, Bernd and Schmid, Matthias and Smith, Adam and Hofner, Benjamin},
 year = {2018},
 pages = {1-15},
 title = {Gradient boosting for distributional regression: faster tuning and improved variable selection via noncyclical updates},
 volume = {28},
 journal = {Statistics and Computing},
 doi = {10.1007/s11222-017-9754-6}
}

@TechReport{Lee2017,
  author	= {Guang-He Lee and Shao-Wen Yang and Tsung-Hsing Lin and Shou-De Lin},
  title		= {Heteroscedastic Learning and its Realization on Matrix Factorization and Gradient Boosting Regression/Classification},
  year		= {2017},
  url		= {https://guanghelee.github.io/pub/kdd17-heteroscedastic-learning.pdf},
}

@techreport{Natarajan2009,
  address = {New York},
  author  = {Ramesh Natarajan},
  institution = {IBM Research Division},
  type    = {Research Report},
  number  = {W0909-085},
  title   = {Gradient Boosting for Joint Regression Modeling of Mean and Dispersion},
  year    = {2009},
}

@inproceedings{brophy2022,
 author = {Brophy, Jonathan and Lowd, Daniel},
 year = {2022},
 pages = {},
 title = {Instance-Based Uncertainty Estimation for Gradient-Boosted Regression Trees},
 booktitle = {Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)}
}

@article{Munir2023,
 author = {Munir, Muhammad Danish},
 year = {2023},
 pages = {14-19},
 title = {Prediction of Heteroscedastic Data Using Linear Regression and Various Machine Learning Models},
 volume = {10},
 number = {1},
 journal = {International Journal of Scientific Research in Mathematical and Statistical Sciences}
}

@article{Friedman2000,
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
 year = {2000},
 pages = {337-407},
 title = {Additive Logistic Regression: A Statistical View of Boosting},
 volume = {28},
 journal = {The Annals of Statistics},
 doi = {10.1214/aos/1016218223}
}

@article{Friedman2001,
 author = {Friedman, Jerome},
 year = {2001},
 pages = {1189–1232},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 number = {5},
 journal = {The Annals of Statistics},
 doi = {10.1214/aos/1013203451}
}

@article{Friedman2002,
 author = {Friedman, Jerome},
 year = {2002},
 pages = {367-378},
 title = {Stochastic Gradient Boosting},
 volume = {38},
 journal = {Computational Statistics \& Data Analysis},
 doi = {10.1016/S0167-9473(01)00065-2}
}

@misc{kaggle,
  author = {Andrew Fogg},
  year = {2016}, 
  title = {Anthony {G}oldbloom gives you the secret to winning {K}aggle competitions},
  howpublished = {Blogpost: https://www.import.io/post/how-to-win-a-kaggle-competition/}
}
@incollection{Schapire2003,
 author="Schapire, Robert E.",
 title="The Boosting Approach to Machine Learning: An Overview",
 booktitle="Nonlinear Estimation and Classification",
 year="2003",
 publisher="Springer New York",
 address="New York, NY",
 pages="149--171",
 abstract="Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost's training error and generalization error; boosting's connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting.",
 isbn="978-0-387-21579-2",
 doi="10.1007/978-0-387-21579-2_9",
 url="https://doi.org/10.1007/978-0-387-21579-2_9"
}


@article{Freund2001,
 author = {Freund, Yoav},
 year = {2001},
 pages = {293--318},
 title = {An Adaptive Version of the Boost by Majority Algorithm},
 volume = {43},
 number = {3},
 journal = {Machine Learning},
 doi = {10.1023/A:1010852229904}
}

@article{Zhenxing2020,
 author = {Zhenxing, Li and Xiaodan, Hong and Hao, Kuangrong and Chen, Lei and Huang, Biao},
 year = {2020},
 pages = {},
 title = {Gaussian Process Regression with heteroscedastic noises – A machine-learning predictive variance approach},
 volume = {157},
 journal = {Chemical Engineering Research and Design},
 doi = {10.1016/j.cherd.2020.02.033}
}

@phdthesis{Henrey2016,
  title={Statistical Learning Tools for Heteroskedastic Data},
  author={Andrew J. Henrey},
  year={2016},
  school = {Simon Fraser University},
  type={PhD thesis},
  url={https://api.semanticscholar.org/CorpusID:63284254}
}

@misc{ruth2016effect,
  title={The Effect of Heteroscedasticity on Regression Trees}, 
  author={Will Ruth and Thomas Loughin},
  year={2016},
  eprint={1606.05273},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@inproceedings{Ustinovskiy2016,
 author = {Ustinovskiy, Yury and Fedorova, Valentina and Gusev, Gleb and Serdyukov, Pavel},
 title = {Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning},
 year = {2016},
 publisher = {JMLR.org},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 pages = {2692–2701},
 numpages = {10},
 location = {New York, NY, USA},
 series = {ICML'16}
}

@article{Wu2021,
 author = {Wu, Zhe and Luo, Junwei and Rincon, David and Christofides, Panagiotis},
 year = {2021},
 pages = {},
 title = {Machine Learning-based Predictive Control Using Noisy Data: Evaluating Performance and Robustness via a Large-Scale Process Simulator},
 volume = {168},
 journal = {Chemical Engineering Research and Design},
 doi = {10.1016/j.cherd.2021.02.011}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: {M}achine learning in {P}ython},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825--2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@inproceedings{Ke2017,
 author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
 title = {Light{GBM}: A Highly Efficient Gradient Boosting Decision Tree},
 year = {2017},
 isbn = {9781510860964},
 publisher = {Curran Associates Inc.},
 address = {Red Hook, NY, USA},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 pages = {3149–3157},
 numpages = {9},
 location = {Long Beach, California, USA},
 series = {NIPS'17}
}

@article{Zhang2023,
 author = {Zhang, Jinxi and Guo, Fan and Hao, Kuangrong and Huang, Biao and Chen, Lei},
 year = {2023},
 pages = {1-10},
 title = {Identification of Errors-in-Variable System With Heteroscedastic Noise and Partially Known Input Using Variational Bayesian},
 volume = {PP},
 journal = {IEEE Transactions on Industrial Informatics},
 doi = {10.1109/TII.2023.3233978}
}
