
@article{Friedman2000,
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
 year = {2000},
 month = {04},
 pages = {337-407},
 title = {Additive Logistic Regression: A Statistical View of Boosting},
 volume = {28},
 journal = {The Annals of Statistics},
 doi = {10.1214/aos/1016218223}
}

@article{Friedman2001,
 author = {Friedman, Jerome},
 year = {2001},
 month = {11},
 pages = {1189–1232},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 number = {5},
 journal = {The Annals of Statistics},
 doi = {10.1214/aos/1013203451}
}

@article{Friedman2002,
 author = {Friedman, Jerome},
 year = {2002},
 month = {02},
 pages = {367-378},
 title = {Stochastic Gradient Boosting},
 volume = {38},
 journal = {Computational Statistics \& Data Analysis},
 doi = {10.1016/S0167-9473(01)00065-2}
}

@misc{kaggle,
  author = {Andrew Fogg},
  year = {2016}, 
  title = {Anthony Goldbloom gives you the secret to winning Kaggle competitions},
  url = {https://www.import.io/post/how-to-win-a-kaggle-competition/}
}

@Inbook{Schapire2003,
 author="Schapire, Robert E.",
 title="The Boosting Approach to Machine Learning: An Overview",
 bookTitle="Nonlinear Estimation and Classification",
 year="2003",
 publisher="Springer New York",
 address="New York, NY",
 pages="149--171",
 abstract="Boosting is a general method for improving the accuracy of any given learning algorithm. Focusing primarily on the AdaBoost algorithm, this chapter overviews some of the recent work on boosting including analyses of AdaBoost's training error and generalization error; boosting's connection to game theory and linear programming; the relationship between boosting and logistic regression; extensions of AdaBoost for multiclass classification problems; methods of incorporating human knowledge into boosting; and experimental and applied work using boosting.",
 isbn="978-0-387-21579-2",
 doi="10.1007/978-0-387-21579-2_9",
 url="https://doi.org/10.1007/978-0-387-21579-2_9"
}


@article{Freund2001,
 author = {Freund, Yoav},
 year = {2001},
 month = {06},
 pages = {293--318},
 title = {An Adaptive Version of the Boost by Majority Algorithm},
 volume = {43},
 number = {3},
 journal = {Machine Learning},
 doi = {10.1023/A:1010852229904}
}

@article{Zhenxing2020,
 author = {Zhenxing, Li and Xiaodan, Hong and Hao, Kuangrong and Chen, Lei and Huang, Biao},
 year = {2020},
 month = {03},
 pages = {},
 title = {Gaussian Process Regression with heteroscedastic noises – A machine-learning predictive variance approach},
 volume = {157},
 journal = {Chemical Engineering Research and Design},
 doi = {10.1016/j.cherd.2020.02.033}
}

@phdthesis{Henrey2016,
  title={Statistical Learning Tools for Heteroskedastic Data},
  author={Andrew J. Henrey},
  year={2016},
  school = {Simon Fraser University},
  type={PhD thesis},
  url={https://api.semanticscholar.org/CorpusID:63284254}
}

@misc{ruth2016effect,
  title={The Effect of Heteroscedasticity on Regression Trees}, 
  author={Will Ruth and Thomas Loughin},
  year={2016},
  eprint={1606.05273},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}

@inproceedings{Ustinovskiy2016,
 author = {Ustinovskiy, Yury and Fedorova, Valentina and Gusev, Gleb and Serdyukov, Pavel},
 title = {Meta-Gradient Boosted Decision Tree Model for Weight and Target Learning},
 year = {2016},
 publisher = {JMLR.org},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 pages = {2692–2701},
 numpages = {10},
 location = {New York, NY, USA},
 series = {ICML'16}
}

@article{Wu2021,
 author = {wu, Zhe and Luo, Junwei and Rincon, David and Christofides, Panagiotis},
 year = {2021},
 month = {02},
 pages = {},
 title = {Machine Learning-based Predictive Control Using Noisy Data: Evaluating Performance and Robustness via a Large-Scale Process Simulator},
 volume = {168},
 journal = {Chemical Engineering Research and Design},
 doi = {10.1016/j.cherd.2021.02.011}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@inproceedings{Ke2017,
 author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
 title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
 year = {2017},
 isbn = {9781510860964},
 publisher = {Curran Associates Inc.},
 address = {Red Hook, NY, USA},
 abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 pages = {3149–3157},
 numpages = {9},
 location = {Long Beach, California, USA},
 series = {NIPS'17}
}

@article{Zhang2023,
 author = {Zhang, Jinxi and Guo, Fan and Hao, Kuangrong and Huang, Biao and Chen, Lei},
 year = {2023},
 month = {10},
 pages = {1-10},
 title = {Identification of Errors-in-Variable System With Heteroscedastic Noise and Partially Known Input Using Variational Bayesian},
 volume = {PP},
 journal = {IEEE Transactions on Industrial Informatics},
 doi = {10.1109/TII.2023.3233978}
}
